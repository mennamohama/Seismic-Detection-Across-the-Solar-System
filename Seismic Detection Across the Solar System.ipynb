{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "\n",
    "def test_data_paths(data_dir, catalog_path):\n",
    "    \"\"\"Test if the paths exist and contain data\"\"\"\n",
    "    #print(\"\\nTesting data paths:\")\n",
    "    #print(f\"Data directory exists: {os.path.exists(data_dir)}\")\n",
    "    #print(f\"Catalog file exists: {os.path.exists(catalog_path)}\")\n",
    "    \n",
    "    if os.path.exists(data_dir):\n",
    "        files = os.listdir(data_dir)\n",
    "        #print(f\"Number of files in data directory: {len(files)}\")\n",
    "        #print(f\"First few files: {files[:5] if files else 'No files found'}\")\n",
    "    \n",
    "    if os.path.exists(catalog_path):\n",
    "        try:\n",
    "            catalog = pd.read_csv(catalog_path)\n",
    "            #print(f\"Catalog shape: {catalog.shape}\")\n",
    "            #print(f\"Catalog columns: {catalog.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading catalog: {str(e)}\")\n",
    "\n",
    "class MoonquakeClassifier:\n",
    "    def __init__(self, data_dir, catalog_path):\n",
    "        \"\"\"Initialize the classifier with paths to data directory and catalog file.\"\"\"\n",
    "        #print(\"\\n=== Initializing MoonquakeClassifier ===\")\n",
    "        self.data_dir = data_dir\n",
    "        #print(f\"Loading catalog from: {catalog_path}\")\n",
    "        self.catalog = pd.read_csv(catalog_path)\n",
    "        #print(f\"Catalog loaded successfully with shape: {self.catalog.shape}\")\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def extract_features(self, velocity_data, time_data):\n",
    "        \"\"\"Extract features from velocity time series data.\"\"\"\n",
    "        #print(\"Extracting features...\")\n",
    "        try:\n",
    "            # Time domain features\n",
    "            time_features = [\n",
    "                np.mean(velocity_data),\n",
    "                np.std(velocity_data),\n",
    "                np.max(np.abs(velocity_data)),\n",
    "                np.min(velocity_data),\n",
    "                np.percentile(velocity_data, 75),\n",
    "                np.percentile(velocity_data, 25),\n",
    "                np.mean(np.abs(np.diff(velocity_data))),\n",
    "                np.std(np.abs(np.diff(velocity_data)))\n",
    "            ]\n",
    "            \n",
    "            # Basic feature check\n",
    "            #print(f\"Time features extracted: {len(time_features)} features\")\n",
    "            \n",
    "            # Frequency domain features\n",
    "            time_step = time_data[1] - time_data[0]\n",
    "            fft_result = np.fft.fft(velocity_data)\n",
    "            frequencies = np.fft.fftfreq(len(velocity_data), d=time_step)\n",
    "            fft_magnitude = np.abs(fft_result)\n",
    "            \n",
    "            low_freq, high_freq = 0.1, 10\n",
    "            fft_filtered = fft_magnitude.copy()\n",
    "            fft_filtered[(frequencies < low_freq) | (frequencies > high_freq)] = 0\n",
    "            \n",
    "            freq_features = [\n",
    "                np.mean(fft_filtered),\n",
    "                np.std(fft_filtered),\n",
    "                np.max(fft_filtered),\n",
    "                np.sum(fft_filtered),\n",
    "                np.median(fft_filtered)\n",
    "            ]\n",
    "            \n",
    "            #print(f\"Frequency features extracted: {len(freq_features)} features\")\n",
    "            \n",
    "            # Spectral features\n",
    "            f, t, Sxx = signal.spectrogram(velocity_data, fs=1/time_step)\n",
    "            spectral_features = [\n",
    "                np.mean(Sxx),\n",
    "                np.std(Sxx),\n",
    "                np.max(Sxx)\n",
    "            ]\n",
    "            \n",
    "            #print(f\"Spectral features extracted: {len(spectral_features)} features\")\n",
    "            \n",
    "            all_features = time_features + freq_features + spectral_features\n",
    "            #print(f\"Total features extracted: {len(all_features)}\")\n",
    "            return all_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def process_single_file(self, file_path):\n",
    "        \"\"\"Process a single CSV file and extract features.\"\"\"\n",
    "        #print(f\"\\nProcessing file: {os.path.basename(file_path)}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            #print(f\"File loaded successfully with shape: {df.shape}\")\n",
    "            \n",
    "            velocity = df['velocity(m/s)'].values\n",
    "            time_rel = df['time_rel(sec)'].values\n",
    "            #print(f\"Velocity data shape: {velocity.shape}\")\n",
    "            \n",
    "            features = self.extract_features(velocity, time_rel)\n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"\n",
    "        Prepare the complete dataset by processing all files and matching with catalog.\n",
    "        Returns:\n",
    "            tuple: (X, y, processed_files) - features, labels, and list of processed filenames\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Preparing Dataset ===\")\n",
    "        X = []\n",
    "        y = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Get list of CSV files\n",
    "        csv_files = [f for f in os.listdir(self.data_dir) if f.endswith('.csv')]\n",
    "        #print(f\"Found {len(csv_files)} CSV files in directory\")\n",
    "        \n",
    "        # Debug: Print first few entries of the catalog\n",
    "        #print(\"\\nFirst few catalog entries:\")\n",
    "        #print(self.catalog['filename'].head())\n",
    "        \n",
    "        for filename in csv_files:\n",
    "            # Normalize filename (remove extension)\n",
    "            normalized_filename = filename.replace('.csv', '')\n",
    "            #print(f\"\\nProcessing {filename} (normalized: {normalized_filename})\")\n",
    "            \n",
    "            # Debug: Print exact matching condition\n",
    "            matching_entries = self.catalog[self.catalog['filename'].str.contains(normalized_filename, regex=False)]\n",
    "            #print(f\"Number of matching entries found: {len(matching_entries)}\")\n",
    "            \n",
    "            if len(matching_entries) > 0:\n",
    "                #print(f\"Found catalog entry for {filename}\")\n",
    "                file_path = os.path.join(self.data_dir, filename)\n",
    "                features = self.process_single_file(file_path)\n",
    "                \n",
    "                if features is not None:\n",
    "                    X.append(features)\n",
    "                    y.append(matching_entries['mq_type'].iloc[0])\n",
    "                    processed_files.append(filename)\n",
    "                    #print(f\"Successfully processed {filename}\")\n",
    "            else:\n",
    "                print(f\"No catalog entry found for {filename}\")\n",
    "        \n",
    "        # Check if dataset is empty after processing all files\n",
    "        if not X:\n",
    "            print(\"No matching files found in the catalog. Dataset is empty.\")\n",
    "            return np.array([]), np.array([]), []\n",
    "        \n",
    "        # Convert to numpy arrays after processing all files\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        return X, y, processed_files\n",
    "\n",
    "    \n",
    "    def train_and_evaluate(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"Train and evaluate the classifier.\"\"\"\n",
    "        print(\"\\n=== Training and Evaluation ===\")\n",
    "        try:\n",
    "            # Prepare dataset\n",
    "            X, y, processed_files = self.prepare_dataset()\n",
    "            \n",
    "            # Print dataset statistics\n",
    "            print(\"\\nDataset Statistics:\")\n",
    "            print(f\"Total samples: {len(X)}\")\n",
    "            print(\"\\nClass distribution:\")\n",
    "            unique_classes, counts = np.unique(y, return_counts=True)\n",
    "            for cls, count in zip(unique_classes, counts):\n",
    "                print(f\"{cls}: {count}\")\n",
    "            \n",
    "            # Encode labels\n",
    "            y_encoded = self.label_encoder.fit_transform(y)\n",
    "            print(f\"Encoded {len(unique_classes)} classes\")\n",
    "            \n",
    "            # Split dataset\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded\n",
    "            )\n",
    "            print(f\"\\nTrain set size: {len(X_train)}\")\n",
    "            print(f\"Test set size: {len(X_test)}\")\n",
    "            \n",
    "            # Scale features\n",
    "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "            X_test_scaled = self.scaler.transform(X_test)\n",
    "            \n",
    "            # Train and evaluate models\n",
    "            models = {\n",
    "                'Random Forest': RandomForestClassifier(\n",
    "                    n_estimators=100, \n",
    "                    max_depth=10,\n",
    "                    random_state=random_state\n",
    "                ),\n",
    "                'Neural Network': MLPClassifier(\n",
    "                    hidden_layer_sizes=(100, 50),\n",
    "                    max_iter=1000,\n",
    "                    random_state=random_state\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            results = {}\n",
    "            for name, model in models.items():\n",
    "                print(f\"\\nTraining {name}...\")\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                print(f\"{name} training completed\")\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                \n",
    "                # Print classification report\n",
    "                print(f\"\\nClassification Report for {name}:\")\n",
    "                report = classification_report(\n",
    "                    y_test,\n",
    "                    y_pred,\n",
    "                    target_names=self.label_encoder.classes_\n",
    "                )\n",
    "                print(f\"\\n{report}\")\n",
    "                \n",
    "                results[name] = {\n",
    "                    'model': model,\n",
    "                    'predictions': y_pred,\n",
    "                    'report': report\n",
    "                }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in training and evaluation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the moonquake classification pipeline.\"\"\"\n",
    "    try:\n",
    "        # Define paths\n",
    "        data_dir = 'C:\\\\Users\\\\DELL\\\\Desktop\\\\New folder\\\\space_apps_2024_seismic_detection\\\\data\\\\lunar\\\\training\\\\data\\\\S12_GradeA'\n",
    "        catalog_path = 'C:\\\\Users\\\\DELL\\\\Desktop\\\\New folder\\\\space_apps_2024_seismic_detection\\\\data\\\\lunar\\\\training\\\\catalogs\\\\apollo12_catalog_GradeA_final.csv'\n",
    "\n",
    "\n",
    "        \n",
    "        # Test paths before proceeding\n",
    "        # print(\"\\n=== Testing Data Paths ===\")\n",
    "        test_data_paths(data_dir, catalog_path)\n",
    "        \n",
    "        # Create instance of classifier\n",
    "        # print(\"\\n=== Creating Classifier Instance ===\")\n",
    "        classifier = MoonquakeClassifier(data_dir, catalog_path)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        # print(\"\\n=== Starting Training and Evaluation ===\")\n",
    "        results = classifier.train_and_evaluate()\n",
    "        \n",
    "        # Print final results\n",
    "        print(\"\\n=== Final Results ===\")\n",
    "        for model_name, model_results in results.items():\n",
    "            print(f\"\\nResults for {model_name}:\")\n",
    "            print(f\"Model: {model_results['model']}\")\n",
    "            print(f\"Classification Report:\\n{model_results['report']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred in main: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "if __name__ == \"main\":\n",
    "    print(\"\\n=== Starting Moonquake Classification Program ===\")\n",
    "    main()\n",
    "    print(\"\\n=== Program Completed ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training and Evaluation ===\n",
      "\n",
      "=== Preparing Dataset ===\n",
      "\n",
      "Dataset Statistics:\n",
      "Total samples: 76\n",
      "\n",
      "Class distribution:\n",
      "deep_mq: 9\n",
      "impact_mq: 64\n",
      "shallow_mq: 3\n",
      "Encoded 3 classes\n",
      "\n",
      "Train set size: 60\n",
      "Test set size: 16\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest training completed\n",
      "\n",
      "Classification Report for Random Forest:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     deep_mq       0.00      0.00      0.00         2\n",
      "   impact_mq       0.79      0.85      0.81        13\n",
      "  shallow_mq       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69        16\n",
      "   macro avg       0.26      0.28      0.27        16\n",
      "weighted avg       0.64      0.69      0.66        16\n",
      "\n",
      "\n",
      "Training Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network training completed\n",
      "\n",
      "Classification Report for Neural Network:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     deep_mq       0.00      0.00      0.00         2\n",
      "   impact_mq       0.77      0.77      0.77        13\n",
      "  shallow_mq       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62        16\n",
      "   macro avg       0.26      0.26      0.26        16\n",
      "weighted avg       0.62      0.62      0.62        16\n",
      "\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "Results for Random Forest:\n",
      "Model: RandomForestClassifier(max_depth=10, random_state=42)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     deep_mq       0.00      0.00      0.00         2\n",
      "   impact_mq       0.79      0.85      0.81        13\n",
      "  shallow_mq       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69        16\n",
      "   macro avg       0.26      0.28      0.27        16\n",
      "weighted avg       0.64      0.69      0.66        16\n",
      "\n",
      "\n",
      "Results for Neural Network:\n",
      "Model: MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     deep_mq       0.00      0.00      0.00         2\n",
      "   impact_mq       0.77      0.77      0.77        13\n",
      "  shallow_mq       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62        16\n",
      "   macro avg       0.26      0.26      0.26        16\n",
      "weighted avg       0.62      0.62      0.62        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Moonquake Classification Program ===\n",
      "\n",
      "=== Training and Evaluation ===\n",
      "\n",
      "Dataset Statistics:\n",
      "Total samples: 76\n",
      "\n",
      "Class distribution:\n",
      "deep_mq: 9\n",
      "impact_mq: 64\n",
      "shallow_mq: 3\n",
      "Encoded 3 classes\n",
      "\n",
      "Train set size: 60\n",
      "Test set size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Transformer Model...\n",
      "Epoch [10/50], Loss: 0.3658\n",
      "Epoch [20/50], Loss: 0.3958\n",
      "Epoch [30/50], Loss: 0.4663\n",
      "Epoch [40/50], Loss: 0.2943\n",
      "Epoch [50/50], Loss: 0.2659\n",
      "Training completed\n",
      "\n",
      "Classification Report for Transformer Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     deep_mq       0.00      0.00      0.00         2\n",
      "   impact_mq       0.81      1.00      0.90        13\n",
      "  shallow_mq       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.81        16\n",
      "   macro avg       0.27      0.33      0.30        16\n",
      "weighted avg       0.66      0.81      0.73        16\n",
      "\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "Results for Transformer:\n",
      "Model: TransformerModel(\n",
      "  (embedding): Linear(in_features=16, out_features=64, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     deep_mq       0.00      0.00      0.00         2\n",
      "   impact_mq       0.81      1.00      0.90        13\n",
      "  shallow_mq       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.81        16\n",
      "   macro avg       0.27      0.33      0.30        16\n",
      "weighted avg       0.66      0.81      0.73        16\n",
      "\n",
      "\n",
      "=== Program Completed ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=64, nhead=4, num_layers=2, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=128, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        return self.fc(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MoonquakeDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class MoonquakeClassifier:\n",
    "    def __init__(self, data_dir, catalog_path):\n",
    "        \"\"\"Initialize the classifier with paths to data directory and catalog file.\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.catalog = pd.read_csv(catalog_path)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def extract_features(self, velocity_data, time_data):\n",
    "        \"\"\"Extract features from velocity time series data.\"\"\"\n",
    "        #print(\"Extracting features...\")\n",
    "        try:\n",
    "            # Time domain features\n",
    "            time_features = [\n",
    "                np.mean(velocity_data),\n",
    "                np.std(velocity_data),\n",
    "                np.max(np.abs(velocity_data)),\n",
    "                np.min(velocity_data),\n",
    "                np.percentile(velocity_data, 75),\n",
    "                np.percentile(velocity_data, 25),\n",
    "                np.mean(np.abs(np.diff(velocity_data))),\n",
    "                np.std(np.abs(np.diff(velocity_data)))\n",
    "            ]\n",
    "            \n",
    "            # Basic feature check\n",
    "            #print(f\"Time features extracted: {len(time_features)} features\")\n",
    "            \n",
    "            # Frequency domain features\n",
    "            time_step = time_data[1] - time_data[0]\n",
    "            fft_result = np.fft.fft(velocity_data)\n",
    "            frequencies = np.fft.fftfreq(len(velocity_data), d=time_step)\n",
    "            fft_magnitude = np.abs(fft_result)\n",
    "            \n",
    "            low_freq, high_freq = 0.1, 10\n",
    "            fft_filtered = fft_magnitude.copy()\n",
    "            fft_filtered[(frequencies < low_freq) | (frequencies > high_freq)] = 0\n",
    "            \n",
    "            freq_features = [\n",
    "                np.mean(fft_filtered),\n",
    "                np.std(fft_filtered),\n",
    "                np.max(fft_filtered),\n",
    "                np.sum(fft_filtered),\n",
    "                np.median(fft_filtered)\n",
    "            ]\n",
    "            \n",
    "            #print(f\"Frequency features extracted: {len(freq_features)} features\")\n",
    "            \n",
    "            # Spectral features\n",
    "            f, t, Sxx = signal.spectrogram(velocity_data, fs=1/time_step)\n",
    "            spectral_features = [\n",
    "                np.mean(Sxx),\n",
    "                np.std(Sxx),\n",
    "                np.max(Sxx)\n",
    "            ]\n",
    "            \n",
    "            #print(f\"Spectral features extracted: {len(spectral_features)} features\")\n",
    "            \n",
    "            all_features = time_features + freq_features + spectral_features\n",
    "            #print(f\"Total features extracted: {len(all_features)}\")\n",
    "            return all_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def process_single_file(self, file_path):\n",
    "        \"\"\"Process a single CSV file and extract features.\"\"\"\n",
    "        #print(f\"\\nProcessing file: {os.path.basename(file_path)}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            #print(f\"File loaded successfully with shape: {df.shape}\")\n",
    "            \n",
    "            velocity = df['velocity(m/s)'].values\n",
    "            time_rel = df['time_rel(sec)'].values\n",
    "            #print(f\"Velocity data shape: {velocity.shape}\")\n",
    "            \n",
    "            features = self.extract_features(velocity, time_rel)\n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"\n",
    "        Prepare the complete dataset by processing all files and matching with catalog.\n",
    "        Returns:\n",
    "            tuple: (X, y, processed_files) - features, labels, and list of processed filenames\n",
    "        \"\"\"\n",
    "        # print(\"\\n=== Preparing Dataset ===\")\n",
    "        X = []\n",
    "        y = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Get list of CSV files\n",
    "        csv_files = [f for f in os.listdir(self.data_dir) if f.endswith('.csv')]\n",
    "        #print(f\"Found {len(csv_files)} CSV files in directory\")\n",
    "        \n",
    "        # Debug: Print first few entries of the catalog\n",
    "        #print(\"\\nFirst few catalog entries:\")\n",
    "        #print(self.catalog['filename'].head())\n",
    "        \n",
    "        for filename in csv_files:\n",
    "            # Normalize filename (remove extension)\n",
    "            normalized_filename = filename.replace('.csv', '')\n",
    "            #print(f\"\\nProcessing {filename} (normalized: {normalized_filename})\")\n",
    "            \n",
    "            # Debug: Print exact matching condition\n",
    "            matching_entries = self.catalog[self.catalog['filename'].str.contains(normalized_filename, regex=False)]\n",
    "            #print(f\"Number of matching entries found: {len(matching_entries)}\")\n",
    "            \n",
    "            if len(matching_entries) > 0:\n",
    "                #print(f\"Found catalog entry for {filename}\")\n",
    "                file_path = os.path.join(self.data_dir, filename)\n",
    "                features = self.process_single_file(file_path)\n",
    "                \n",
    "                if features is not None:\n",
    "                    X.append(features)\n",
    "                    y.append(matching_entries['mq_type'].iloc[0])\n",
    "                    processed_files.append(filename)\n",
    "                    #print(f\"Successfully processed {filename}\")\n",
    "            else:\n",
    "                print(f\"No catalog entry found for {filename}\")\n",
    "        \n",
    "        # Check if dataset is empty after processing all files\n",
    "        if not X:\n",
    "            print(\"No matching files found in the catalog. Dataset is empty.\")\n",
    "            return np.array([]), np.array([]), []\n",
    "        \n",
    "        # Convert to numpy arrays after processing all files\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # print(f\"\\nDataset preparation completed:\")\n",
    "        # print(f\"X shape: {X.shape}\")\n",
    "        # print(f\"y shape: {y.shape}\")\n",
    "        # print(f\"Processed {len(processed_files)} files successfully\")\n",
    "        \n",
    "        return X, y, processed_files\n",
    "    def train_and_evaluate(self, test_size=0.2, random_state=42, batch_size=32, num_epochs=50):\n",
    "        print(\"\\n=== Training and Evaluation ===\")\n",
    "        try:\n",
    "            X, y, processed_files = self.prepare_dataset()\n",
    "            \n",
    "            print(\"\\nDataset Statistics:\")\n",
    "            print(f\"Total samples: {len(X)}\")\n",
    "            print(\"\\nClass distribution:\")\n",
    "            unique_classes, counts = np.unique(y, return_counts=True)\n",
    "            for cls, count in zip(unique_classes, counts):\n",
    "                print(f\"{cls}: {count}\")\n",
    "            \n",
    "            y_encoded = self.label_encoder.fit_transform(y)\n",
    "            print(f\"Encoded {len(unique_classes)} classes\")\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded\n",
    "            )\n",
    "            print(f\"\\nTrain set size: {len(X_train)}\")\n",
    "            print(f\"Test set size: {len(X_test)}\")\n",
    "            \n",
    "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "            X_test_scaled = self.scaler.transform(X_test)\n",
    "            \n",
    "            train_dataset = MoonquakeDataset(X_train_scaled, y_train)\n",
    "            test_dataset = MoonquakeDataset(X_test_scaled, y_test)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "            \n",
    "            model = TransformerModel(input_dim=X_train_scaled.shape[1], num_classes=len(unique_classes)).to(self.device)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            \n",
    "            print(\"\\nTraining Transformer Model...\")\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                for batch_features, batch_labels in train_loader:\n",
    "                    batch_features, batch_labels = batch_features.to(self.device), batch_labels.to(self.device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_features.unsqueeze(1))  # Add sequence dimension\n",
    "                    loss = criterion(outputs, batch_labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            print(\"Training completed\")\n",
    "            \n",
    "            model.eval()\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for batch_features, batch_labels in test_loader:\n",
    "                    batch_features, batch_labels = batch_features.to(self.device), batch_labels.to(self.device)\n",
    "                    outputs = model(batch_features.unsqueeze(1))\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(batch_labels.cpu().numpy())\n",
    "            \n",
    "            print(\"\\nClassification Report for Transformer Model:\")\n",
    "            report = classification_report(\n",
    "                all_labels,\n",
    "                all_preds,\n",
    "                target_names=self.label_encoder.classes_\n",
    "            )\n",
    "            print(report)\n",
    "            \n",
    "            return {'Transformer': {'model': model, 'report': report}}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in training and evaluation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the moonquake classification pipeline.\"\"\"\n",
    "    # Define paths\n",
    "    data_dir = 'C:\\\\Users\\\\DELL\\\\Desktop\\\\New folder\\\\space_apps_2024_seismic_detection\\\\data\\\\lunar\\\\training\\\\data\\\\S12_GradeA'\n",
    "    catalog_path = 'C:\\\\Users\\\\DELL\\\\Desktop\\\\New folder\\\\space_apps_2024_seismic_detection\\\\data\\\\lunar\\\\training\\\\catalogs\\\\apollo12_catalog_GradeA_final.csv'\n",
    "    \n",
    "    classifier = MoonquakeClassifier(data_dir, catalog_path)\n",
    "    results = classifier.train_and_evaluate()\n",
    "    \n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    for model_name, model_results in results.items():\n",
    "        print(f\"\\nResults for {model_name}:\")\n",
    "        print(f\"Model: {model_results['model']}\")\n",
    "        print(f\"Classification Report:\\n{model_results['report']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== Starting Moonquake Classification Program ===\")\n",
    "    main()\n",
    "    print(\"\\n=== Program Completed ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://dec0cff370893434bb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://dec0cff370893434bb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "\n",
    "def test_data_paths(data_dir, catalog_path):\n",
    "    \"\"\"Test if the paths exist and contain data\"\"\"\n",
    "    #print(\"\\nTesting data paths:\")\n",
    "    #print(f\"Data directory exists: {os.path.exists(data_dir)}\")\n",
    "    #print(f\"Catalog file exists: {os.path.exists(catalog_path)}\")\n",
    "    \n",
    "    if os.path.exists(data_dir):\n",
    "        files = os.listdir(data_dir)\n",
    "        #print(f\"Number of files in data directory: {len(files)}\")\n",
    "        #print(f\"First few files: {files[:5] if files else 'No files found'}\")\n",
    "    \n",
    "    if os.path.exists(catalog_path):\n",
    "        try:\n",
    "            catalog = pd.read_csv(catalog_path)\n",
    "            #print(f\"Catalog shape: {catalog.shape}\")\n",
    "            #print(f\"Catalog columns: {catalog.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading catalog: {str(e)}\")\n",
    "\n",
    "class MoonquakeClassifier:\n",
    "    def __init__(self, data_dir, catalog_path):\n",
    "        \"\"\"Initialize the classifier with paths to data directory and catalog file.\"\"\"\n",
    "        #print(\"\\n=== Initializing MoonquakeClassifier ===\")\n",
    "        self.data_dir = data_dir\n",
    "        #print(f\"Loading catalog from: {catalog_path}\")\n",
    "        self.catalog = pd.read_csv(catalog_path)\n",
    "        #print(f\"Catalog loaded successfully with shape: {self.catalog.shape}\")\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def extract_features(self, velocity_data, time_data):\n",
    "        \"\"\"Extract features from velocity time series data.\"\"\"\n",
    "        #print(\"Extracting features...\")\n",
    "        try:\n",
    "            # Time domain features\n",
    "            time_features = [\n",
    "                np.mean(velocity_data),\n",
    "                np.std(velocity_data),\n",
    "                np.max(np.abs(velocity_data)),\n",
    "                np.min(velocity_data),\n",
    "                np.percentile(velocity_data, 75),\n",
    "                np.percentile(velocity_data, 25),\n",
    "                np.mean(np.abs(np.diff(velocity_data))),\n",
    "                np.std(np.abs(np.diff(velocity_data)))\n",
    "            ]\n",
    "            \n",
    "            # Basic feature check\n",
    "            #print(f\"Time features extracted: {len(time_features)} features\")\n",
    "            \n",
    "            # Frequency domain features\n",
    "            time_step = time_data[1] - time_data[0]\n",
    "            fft_result = np.fft.fft(velocity_data)\n",
    "            frequencies = np.fft.fftfreq(len(velocity_data), d=time_step)\n",
    "            fft_magnitude = np.abs(fft_result)\n",
    "            \n",
    "            low_freq, high_freq = 0.1, 10\n",
    "            fft_filtered = fft_magnitude.copy()\n",
    "            fft_filtered[(frequencies < low_freq) | (frequencies > high_freq)] = 0\n",
    "            \n",
    "            freq_features = [\n",
    "                np.mean(fft_filtered),\n",
    "                np.std(fft_filtered),\n",
    "                np.max(fft_filtered),\n",
    "                np.sum(fft_filtered),\n",
    "                np.median(fft_filtered)\n",
    "            ]\n",
    "            \n",
    "            #print(f\"Frequency features extracted: {len(freq_features)} features\")\n",
    "            \n",
    "            # Spectral features\n",
    "            f, t, Sxx = signal.spectrogram(velocity_data, fs=1/time_step)\n",
    "            spectral_features = [\n",
    "                np.mean(Sxx),\n",
    "                np.std(Sxx),\n",
    "                np.max(Sxx)\n",
    "            ]\n",
    "            \n",
    "            #print(f\"Spectral features extracted: {len(spectral_features)} features\")\n",
    "            \n",
    "            all_features = time_features + freq_features + spectral_features\n",
    "            #print(f\"Total features extracted: {len(all_features)}\")\n",
    "            return all_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def process_single_file(self, file_path):\n",
    "        \"\"\"Process a single CSV file and extract features.\"\"\"\n",
    "        #print(f\"\\nProcessing file: {os.path.basename(file_path)}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            #print(f\"File loaded successfully with shape: {df.shape}\")\n",
    "            \n",
    "            velocity = df['velocity(m/s)'].values\n",
    "            time_rel = df['time_rel(sec)'].values\n",
    "            #print(f\"Velocity data shape: {velocity.shape}\")\n",
    "            \n",
    "            features = self.extract_features(velocity, time_rel)\n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"\n",
    "        Prepare the complete dataset by processing all files and matching with catalog.\n",
    "        Returns:\n",
    "            tuple: (X, y, processed_files) - features, labels, and list of processed filenames\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Preparing Dataset ===\")\n",
    "        X = []\n",
    "        y = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Get list of CSV files\n",
    "        csv_files = [f for f in os.listdir(self.data_dir) if f.endswith('.csv')]\n",
    "        #print(f\"Found {len(csv_files)} CSV files in directory\")\n",
    "        \n",
    "        # Debug: Print first few entries of the catalog\n",
    "        #print(\"\\nFirst few catalog entries:\")\n",
    "        #print(self.catalog['filename'].head())\n",
    "        \n",
    "        for filename in csv_files:\n",
    "            # Normalize filename (remove extension)\n",
    "            normalized_filename = filename.replace('.csv', '')\n",
    "            #print(f\"\\nProcessing {filename} (normalized: {normalized_filename})\")\n",
    "            \n",
    "            # Debug: Print exact matching condition\n",
    "            matching_entries = self.catalog[self.catalog['filename'].str.contains(normalized_filename, regex=False)]\n",
    "            #print(f\"Number of matching entries found: {len(matching_entries)}\")\n",
    "            \n",
    "            if len(matching_entries) > 0:\n",
    "                #print(f\"Found catalog entry for {filename}\")\n",
    "                file_path = os.path.join(self.data_dir, filename)\n",
    "                features = self.process_single_file(file_path)\n",
    "                \n",
    "                if features is not None:\n",
    "                    X.append(features)\n",
    "                    y.append(matching_entries['mq_type'].iloc[0])\n",
    "                    processed_files.append(filename)\n",
    "                    #print(f\"Successfully processed {filename}\")\n",
    "            else:\n",
    "                print(f\"No catalog entry found for {filename}\")\n",
    "        \n",
    "        # Check if dataset is empty after processing all files\n",
    "        if not X:\n",
    "            print(\"No matching files found in the catalog. Dataset is empty.\")\n",
    "            return np.array([]), np.array([]), []\n",
    "        \n",
    "        # Convert to numpy arrays after processing all files\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        return X, y, processed_files\n",
    "\n",
    "    \n",
    "    def train_and_evaluate(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"Train and evaluate the classifier.\"\"\"\n",
    "        print(\"\\n=== Training and Evaluation ===\")\n",
    "        try:\n",
    "            # Prepare dataset\n",
    "            X, y, processed_files = self.prepare_dataset()\n",
    "            \n",
    "            # Print dataset statistics\n",
    "            print(\"\\nDataset Statistics:\")\n",
    "            print(f\"Total samples: {len(X)}\")\n",
    "            print(\"\\nClass distribution:\")\n",
    "            unique_classes, counts = np.unique(y, return_counts=True)\n",
    "            for cls, count in zip(unique_classes, counts):\n",
    "                print(f\"{cls}: {count}\")\n",
    "            \n",
    "            # Encode labels\n",
    "            y_encoded = self.label_encoder.fit_transform(y)\n",
    "            print(f\"Encoded {len(unique_classes)} classes\")\n",
    "            \n",
    "            # Split dataset\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded\n",
    "            )\n",
    "            print(f\"\\nTrain set size: {len(X_train)}\")\n",
    "            print(f\"Test set size: {len(X_test)}\")\n",
    "            \n",
    "            # Scale features\n",
    "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "            X_test_scaled = self.scaler.transform(X_test)\n",
    "            \n",
    "            # Train and evaluate models\n",
    "            models = {\n",
    "                'Random Forest': RandomForestClassifier(\n",
    "                    n_estimators=100, \n",
    "                    max_depth=10,\n",
    "                    random_state=random_state\n",
    "                ),\n",
    "                'Neural Network': MLPClassifier(\n",
    "                    hidden_layer_sizes=(100, 50),\n",
    "                    max_iter=1000,\n",
    "                    random_state=random_state\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            results = {}\n",
    "            for name, model in models.items():\n",
    "                print(f\"\\nTraining {name}...\")\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                print(f\"{name} training completed\")\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                \n",
    "                # Print classification report\n",
    "                print(f\"\\nClassification Report for {name}:\")\n",
    "                report = classification_report(\n",
    "                    y_test,\n",
    "                    y_pred,\n",
    "                    target_names=self.label_encoder.classes_\n",
    "                )\n",
    "                print(f\"\\n{report}\")\n",
    "                \n",
    "                results[name] = {\n",
    "                    'model': model,\n",
    "                    'predictions': y_pred,\n",
    "                    'report': report\n",
    "                }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in training and evaluation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the moonquake classification pipeline.\"\"\"\n",
    "    try:\n",
    "        # Define paths\n",
    "        data_dir = 'C:\\\\Users\\\\DELL\\\\Desktop\\\\New folder\\\\space_apps_2024_seismic_detection\\\\data\\\\lunar\\\\training\\\\data\\\\S12_GradeA'\n",
    "        catalog_path = 'C:\\\\Users\\\\DELL\\\\Desktop\\\\New folder\\\\space_apps_2024_seismic_detection\\\\data\\\\lunar\\\\training\\\\catalogs\\\\apollo12_catalog_GradeA_final.csv'\n",
    "\n",
    "\n",
    "        \n",
    "        # Test paths before proceeding\n",
    "        # print(\"\\n=== Testing Data Paths ===\")\n",
    "        test_data_paths(data_dir, catalog_path)\n",
    "        \n",
    "        # Create instance of classifier\n",
    "        # print(\"\\n=== Creating Classifier Instance ===\")\n",
    "        classifier = MoonquakeClassifier(data_dir, catalog_path)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        # print(\"\\n=== Starting Training and Evaluation ===\")\n",
    "        results = classifier.train_and_evaluate()\n",
    "        \n",
    "        # Print final results\n",
    "        print(\"\\n=== Final Results ===\")\n",
    "        for model_name, model_results in results.items():\n",
    "            print(f\"\\nResults for {model_name}:\")\n",
    "            print(f\"Model: {model_results['model']}\")\n",
    "            print(f\"Classification Report:\\n{model_results['report']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred in main: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def process_files(data_dir_file, catalog_file):\n",
    "    \"\"\"\n",
    "    Process the uploaded files and run the moonquake classification\n",
    "    \n",
    "    Args:\n",
    "        data_dir_file: Uploaded ZIP file containing CSV data files\n",
    "        catalog_file: Uploaded catalog CSV file\n",
    "    \n",
    "    Returns:\n",
    "        str: Classification results as formatted text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create temporary directory for processing\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Save catalog file\n",
    "            catalog_path = os.path.join(temp_dir, \"catalog.csv\")\n",
    "            catalog_file.save(catalog_path)\n",
    "            \n",
    "            # Create directory for data files\n",
    "            data_dir = os.path.join(temp_dir, \"data\")\n",
    "            os.makedirs(data_dir, exist_ok=True)\n",
    "            \n",
    "            # Extract data files\n",
    "            if data_dir_file.name.endswith('.zip'):\n",
    "                import zipfile\n",
    "                with zipfile.ZipFile(data_dir_file.name, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(data_dir)\n",
    "            else:\n",
    "                return \"Please upload a ZIP file containing the CSV data files\"\n",
    "\n",
    "            # Test paths\n",
    "            test_data_paths(data_dir, catalog_path)\n",
    "            \n",
    "            # Initialize and run classifier\n",
    "            classifier = MoonquakeClassifier(data_dir, catalog_path)\n",
    "            results = classifier.train_and_evaluate()\n",
    "            \n",
    "            # Format results\n",
    "            output_text = \"=== Moonquake Classification Results ===\\n\\n\"\n",
    "            for model_name, model_results in results.items():\n",
    "                output_text += f\"\\nResults for {model_name}:\\n\"\n",
    "                output_text += f\"Classification Report:\\n{model_results['report']}\\n\"\n",
    "                output_text += \"-\" * 50 + \"\\n\"\n",
    "            \n",
    "            return output_text\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Create Gradio interface\n",
    "def create_interface():\n",
    "    with gr.Blocks(title=\"Moonquake Classifier\") as interface:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # Moonquake Classification Interface\n",
    "        \n",
    "        Upload your data files to classify moonquake events.\n",
    "        \n",
    "        **Instructions:**\n",
    "        1. Upload a ZIP file containing your CSV data files\n",
    "        2. Upload your catalog CSV file\n",
    "        3. Click 'Submit' to run the classification\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            data_dir_input = gr.File(\n",
    "                label=\"Data Directory (ZIP file containing CSV files)\",\n",
    "                file_types=[\".zip\"]\n",
    "            )\n",
    "            catalog_input = gr.File(\n",
    "                label=\"Catalog File (CSV)\",\n",
    "                file_types=[\".csv\"]\n",
    "            )\n",
    "            \n",
    "        submit_btn = gr.Button(\"Submit\")\n",
    "        \n",
    "        output = gr.Textbox(\n",
    "            label=\"Classification Results\",\n",
    "            lines=20,\n",
    "            max_lines=30\n",
    "        )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            fn=process_files,\n",
    "            inputs=[data_dir_input, catalog_input],\n",
    "            outputs=output\n",
    "        )\n",
    "        \n",
    "    return interface\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    interface = create_interface()\n",
    "    interface.launch(\n",
    "        share=True,\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.3.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gradio) (4.4.0)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.3-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.4.2 (from gradio)\n",
      "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting huggingface-hub>=0.25.1 (from gradio)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gradio) (3.1.4)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gradio) (3.10.9)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gradio) (2.1.4)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gradio) (2.9.0)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pyyaml<7.0,>=5.0 (from gradio)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.7.1-py3-none-win_amd64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.41.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Using cached tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Using cached typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gradio) (0.30.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gradio-client==1.4.2->gradio) (2024.9.0)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio)\n",
      "  Downloading websockets-12.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5.0,>=3.0->gradio) (3.8)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic>=2.0->gradio) (2.23.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading gradio-5.3.0-py3-none-any.whl (56.7 MB)\n",
      "   ---------------------------------------- 0.0/56.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/56.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/56.7 MB 2.4 MB/s eta 0:00:24\n",
      "    --------------------------------------- 1.3/56.7 MB 2.5 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 1.6/56.7 MB 2.3 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 2.1/56.7 MB 2.3 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 2.4/56.7 MB 2.2 MB/s eta 0:00:25\n",
      "   -- ------------------------------------- 2.9/56.7 MB 2.2 MB/s eta 0:00:25\n",
      "   -- ------------------------------------- 3.4/56.7 MB 2.1 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 3.7/56.7 MB 2.0 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 3.9/56.7 MB 2.0 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 4.2/56.7 MB 1.9 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 4.5/56.7 MB 1.8 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 4.7/56.7 MB 1.8 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 5.0/56.7 MB 1.8 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 5.2/56.7 MB 1.7 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 5.5/56.7 MB 1.7 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 5.8/56.7 MB 1.7 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 6.0/56.7 MB 1.6 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 6.3/56.7 MB 1.6 MB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 6.6/56.7 MB 1.6 MB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 6.8/56.7 MB 1.6 MB/s eta 0:00:33\n",
      "   ---- ----------------------------------- 7.1/56.7 MB 1.5 MB/s eta 0:00:33\n",
      "   ---- ----------------------------------- 7.1/56.7 MB 1.5 MB/s eta 0:00:33\n",
      "   ----- ---------------------------------- 7.6/56.7 MB 1.5 MB/s eta 0:00:33\n",
      "   ----- ---------------------------------- 7.9/56.7 MB 1.5 MB/s eta 0:00:33\n",
      "   ----- ---------------------------------- 8.1/56.7 MB 1.5 MB/s eta 0:00:33\n",
      "   ----- ---------------------------------- 8.4/56.7 MB 1.5 MB/s eta 0:00:33\n",
      "   ------ --------------------------------- 8.7/56.7 MB 1.5 MB/s eta 0:00:33\n",
      "   ------ --------------------------------- 8.9/56.7 MB 1.5 MB/s eta 0:00:33\n",
      "   ------ --------------------------------- 9.2/56.7 MB 1.5 MB/s eta 0:00:33\n",
      "   ------ --------------------------------- 9.4/56.7 MB 1.5 MB/s eta 0:00:32\n",
      "   ------ --------------------------------- 9.7/56.7 MB 1.5 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 10.0/56.7 MB 1.5 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 10.5/56.7 MB 1.5 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 10.7/56.7 MB 1.5 MB/s eta 0:00:31\n",
      "   ------- -------------------------------- 11.0/56.7 MB 1.5 MB/s eta 0:00:31\n",
      "   -------- ------------------------------- 11.5/56.7 MB 1.5 MB/s eta 0:00:31\n",
      "   -------- ------------------------------- 11.8/56.7 MB 1.5 MB/s eta 0:00:30\n",
      "   -------- ------------------------------- 12.3/56.7 MB 1.5 MB/s eta 0:00:30\n",
      "   -------- ------------------------------- 12.6/56.7 MB 1.5 MB/s eta 0:00:29\n",
      "   --------- ------------------------------ 13.1/56.7 MB 1.5 MB/s eta 0:00:29\n",
      "   --------- ------------------------------ 13.6/56.7 MB 1.6 MB/s eta 0:00:28\n",
      "   --------- ------------------------------ 13.9/56.7 MB 1.6 MB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 14.4/56.7 MB 1.6 MB/s eta 0:00:27\n",
      "   ---------- ----------------------------- 14.7/56.7 MB 1.6 MB/s eta 0:00:27\n",
      "   ---------- ----------------------------- 14.9/56.7 MB 1.6 MB/s eta 0:00:27\n",
      "   ---------- ----------------------------- 15.5/56.7 MB 1.6 MB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 15.7/56.7 MB 1.6 MB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 16.3/56.7 MB 1.6 MB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 16.8/56.7 MB 1.6 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 17.0/56.7 MB 1.6 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 17.6/56.7 MB 1.6 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 17.8/56.7 MB 1.6 MB/s eta 0:00:24\n",
      "   ------------ --------------------------- 18.4/56.7 MB 1.6 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 18.9/56.7 MB 1.6 MB/s eta 0:00:23\n",
      "   ------------- -------------------------- 19.1/56.7 MB 1.6 MB/s eta 0:00:23\n",
      "   ------------- -------------------------- 19.4/56.7 MB 1.6 MB/s eta 0:00:23\n",
      "   ------------- -------------------------- 19.7/56.7 MB 1.6 MB/s eta 0:00:23\n",
      "   -------------- ------------------------- 20.2/56.7 MB 1.6 MB/s eta 0:00:23\n",
      "   -------------- ------------------------- 20.4/56.7 MB 1.6 MB/s eta 0:00:23\n",
      "   -------------- ------------------------- 21.0/56.7 MB 1.6 MB/s eta 0:00:22\n",
      "   -------------- ------------------------- 21.2/56.7 MB 1.6 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 21.8/56.7 MB 1.7 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 22.0/56.7 MB 1.7 MB/s eta 0:00:21\n",
      "   --------------- ------------------------ 22.3/56.7 MB 1.7 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 22.8/56.7 MB 1.7 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 23.1/56.7 MB 1.7 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 23.6/56.7 MB 1.7 MB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 23.9/56.7 MB 1.7 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 24.4/56.7 MB 1.7 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 24.6/56.7 MB 1.7 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 25.2/56.7 MB 1.7 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 25.4/56.7 MB 1.7 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 25.7/56.7 MB 1.7 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 26.0/56.7 MB 1.7 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 26.2/56.7 MB 1.7 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 26.5/56.7 MB 1.7 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 26.7/56.7 MB 1.6 MB/s eta 0:00:19\n",
      "   ------------------- -------------------- 27.0/56.7 MB 1.6 MB/s eta 0:00:19\n",
      "   ------------------- -------------------- 27.3/56.7 MB 1.6 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 27.5/56.7 MB 1.6 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 27.8/56.7 MB 1.6 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 28.0/56.7 MB 1.6 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 28.3/56.7 MB 1.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 28.6/56.7 MB 1.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 28.8/56.7 MB 1.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 29.1/56.7 MB 1.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 29.4/56.7 MB 1.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 29.6/56.7 MB 1.6 MB/s eta 0:00:17\n",
      "   --------------------- ------------------ 29.9/56.7 MB 1.6 MB/s eta 0:00:17\n",
      "   --------------------- ------------------ 30.4/56.7 MB 1.6 MB/s eta 0:00:17\n",
      "   --------------------- ------------------ 30.7/56.7 MB 1.6 MB/s eta 0:00:17\n",
      "   --------------------- ------------------ 30.9/56.7 MB 1.6 MB/s eta 0:00:17\n",
      "   ---------------------- ----------------- 31.5/56.7 MB 1.6 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 31.7/56.7 MB 1.6 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 32.0/56.7 MB 1.6 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 32.5/56.7 MB 1.6 MB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 32.8/56.7 MB 1.6 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 33.0/56.7 MB 1.6 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 33.6/56.7 MB 1.6 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 34.1/56.7 MB 1.6 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 34.3/56.7 MB 1.6 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 34.6/56.7 MB 1.6 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 35.1/56.7 MB 1.6 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 35.4/56.7 MB 1.6 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 35.9/56.7 MB 1.6 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 36.2/56.7 MB 1.6 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 36.7/56.7 MB 1.6 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 37.0/56.7 MB 1.6 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 37.5/56.7 MB 1.6 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 37.7/56.7 MB 1.6 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 38.3/56.7 MB 1.6 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 38.3/56.7 MB 1.6 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 38.8/56.7 MB 1.6 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 39.1/56.7 MB 1.6 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 39.3/56.7 MB 1.6 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 39.8/56.7 MB 1.6 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 40.1/56.7 MB 1.6 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 40.4/56.7 MB 1.6 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 40.6/56.7 MB 1.6 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 40.9/56.7 MB 1.6 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 41.4/56.7 MB 1.6 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 41.7/56.7 MB 1.6 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 41.9/56.7 MB 1.6 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 42.2/56.7 MB 1.6 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 42.7/56.7 MB 1.6 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 43.0/56.7 MB 1.6 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 43.3/56.7 MB 1.6 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 43.8/56.7 MB 1.6 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 44.0/56.7 MB 1.6 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 44.3/56.7 MB 1.6 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 44.6/56.7 MB 1.6 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 44.8/56.7 MB 1.6 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 45.1/56.7 MB 1.6 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 45.4/56.7 MB 1.6 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 45.6/56.7 MB 1.6 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 45.6/56.7 MB 1.6 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 45.9/56.7 MB 1.6 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 46.1/56.7 MB 1.6 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 46.4/56.7 MB 1.6 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 46.7/56.7 MB 1.6 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 46.9/56.7 MB 1.6 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 46.9/56.7 MB 1.6 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 47.2/56.7 MB 1.6 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 47.4/56.7 MB 1.6 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 47.7/56.7 MB 1.6 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 48.0/56.7 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 48.2/56.7 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 48.8/56.7 MB 1.5 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 49.0/56.7 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 49.3/56.7 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 49.5/56.7 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 50.1/56.7 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 50.3/56.7 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 50.6/56.7 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 50.9/56.7 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 51.4/56.7 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 51.6/56.7 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 52.2/56.7 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 52.4/56.7 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 53.0/56.7 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 53.2/56.7 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 53.5/56.7 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 54.0/56.7 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 54.3/56.7 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 54.8/56.7 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  55.3/56.7 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  55.8/56.7 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.1/56.7 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.4/56.7 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 56.7/56.7 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n",
      "Using cached tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.115.3-py3-none-any.whl (94 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Downloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Downloading ruff-0.7.1-py3-none-win_amd64.whl (9.4 MB)\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.4 MB 1.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/9.4 MB 1.6 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.3/9.4 MB 1.7 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.6/9.4 MB 1.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.8/9.4 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.4/9.4 MB 1.6 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 2.6/9.4 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 2.9/9.4 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 3.1/9.4 MB 1.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 3.4/9.4 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.7/9.4 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.9/9.4 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.9/9.4 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.2/9.4 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.2/9.4 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 4.5/9.4 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 4.5/9.4 MB 1.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 4.7/9.4 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 5.0/9.4 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 5.0/9.4 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 5.2/9.4 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 5.2/9.4 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 5.5/9.4 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 5.8/9.4 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 5.8/9.4 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 6.0/9.4 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 6.3/9.4 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 6.6/9.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 6.8/9.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 6.8/9.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 7.1/9.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 7.3/9.4 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 7.6/9.4 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 7.9/9.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.1/9.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.1/9.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.4/9.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.9/9.4 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.2/9.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.4/9.4 MB 1.1 MB/s eta 0:00:00\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.41.0-py3-none-any.whl (73 kB)\n",
      "Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Using cached ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading websockets-12.0-cp311-cp311-win_amd64.whl (124 kB)\n",
      "Installing collected packages: pydub, websockets, tomlkit, shellingham, semantic-version, ruff, pyyaml, python-multipart, markupsafe, httpcore, ffmpy, aiofiles, starlette, huggingface-hub, httpx, typer, gradio-client, fastapi, gradio\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.38.4\n",
      "    Uninstalling starlette-0.38.4:\n",
      "      Successfully uninstalled starlette-0.38.4\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.113.0\n",
      "    Uninstalling fastapi-0.113.0:\n",
      "      Successfully uninstalled fastapi-0.113.0\n",
      "Successfully installed aiofiles-23.2.1 fastapi-0.115.3 ffmpy-0.4.0 gradio-5.3.0 gradio-client-1.4.2 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.26.1 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 pyyaml-6.0.2 ruff-0.7.1 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.41.0 tomlkit-0.12.0 typer-0.12.5 websockets-12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7860): only one usage of each socket address (protocol/network address/port) is normally permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on public URL: https://fb816b187c51b99093.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://fb816b187c51b99093.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "import shutil\n",
    "\n",
    "def test_data_paths(data_dir, catalog_path):\n",
    "    \"\"\"Test if the paths exist and contain data\"\"\"\n",
    "    #print(\"\\nTesting data paths:\")\n",
    "    #print(f\"Data directory exists: {os.path.exists(data_dir)}\")\n",
    "    #print(f\"Catalog file exists: {os.path.exists(catalog_path)}\")\n",
    "    \n",
    "    if os.path.exists(data_dir):\n",
    "        files = os.listdir(data_dir)\n",
    "        #print(f\"Number of files in data directory: {len(files)}\")\n",
    "        #print(f\"First few files: {files[:5] if files else 'No files found'}\")\n",
    "    \n",
    "    if os.path.exists(catalog_path):\n",
    "        try:\n",
    "            catalog = pd.read_csv(catalog_path)\n",
    "            #print(f\"Catalog shape: {catalog.shape}\")\n",
    "            #print(f\"Catalog columns: {catalog.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading catalog: {str(e)}\")\n",
    "\n",
    "class MoonquakeClassifier:\n",
    "    def __init__(self, data_dir, catalog_path):\n",
    "        \"\"\"Initialize the classifier with paths to data directory and catalog file.\"\"\"\n",
    "        #print(\"\\n=== Initializing MoonquakeClassifier ===\")\n",
    "        self.data_dir = data_dir\n",
    "        #print(f\"Loading catalog from: {catalog_path}\")\n",
    "        self.catalog = pd.read_csv(catalog_path)\n",
    "        #print(f\"Catalog loaded successfully with shape: {self.catalog.shape}\")\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def extract_features(self, velocity_data, time_data):\n",
    "        \"\"\"Extract features from velocity time series data.\"\"\"\n",
    "        #print(\"Extracting features...\")\n",
    "        try:\n",
    "            # Time domain features\n",
    "            time_features = [\n",
    "                np.mean(velocity_data),\n",
    "                np.std(velocity_data),\n",
    "                np.max(np.abs(velocity_data)),\n",
    "                np.min(velocity_data),\n",
    "                np.percentile(velocity_data, 75),\n",
    "                np.percentile(velocity_data, 25),\n",
    "                np.mean(np.abs(np.diff(velocity_data))),\n",
    "                np.std(np.abs(np.diff(velocity_data)))\n",
    "            ]\n",
    "            \n",
    "            # Basic feature check\n",
    "            #print(f\"Time features extracted: {len(time_features)} features\")\n",
    "            \n",
    "            # Frequency domain features\n",
    "            time_step = time_data[1] - time_data[0]\n",
    "            fft_result = np.fft.fft(velocity_data)\n",
    "            frequencies = np.fft.fftfreq(len(velocity_data), d=time_step)\n",
    "            fft_magnitude = np.abs(fft_result)\n",
    "            \n",
    "            low_freq, high_freq = 0.1, 10\n",
    "            fft_filtered = fft_magnitude.copy()\n",
    "            fft_filtered[(frequencies < low_freq) | (frequencies > high_freq)] = 0\n",
    "            \n",
    "            freq_features = [\n",
    "                np.mean(fft_filtered),\n",
    "                np.std(fft_filtered),\n",
    "                np.max(fft_filtered),\n",
    "                np.sum(fft_filtered),\n",
    "                np.median(fft_filtered)\n",
    "            ]\n",
    "            \n",
    "            #print(f\"Frequency features extracted: {len(freq_features)} features\")\n",
    "            \n",
    "            # Spectral features\n",
    "            f, t, Sxx = signal.spectrogram(velocity_data, fs=1/time_step)\n",
    "            spectral_features = [\n",
    "                np.mean(Sxx),\n",
    "                np.std(Sxx),\n",
    "                np.max(Sxx)\n",
    "            ]\n",
    "            \n",
    "            #print(f\"Spectral features extracted: {len(spectral_features)} features\")\n",
    "            \n",
    "            all_features = time_features + freq_features + spectral_features\n",
    "            #print(f\"Total features extracted: {len(all_features)}\")\n",
    "            return all_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def process_single_file(self, file_path):\n",
    "        \"\"\"Process a single CSV file and extract features.\"\"\"\n",
    "        #print(f\"\\nProcessing file: {os.path.basename(file_path)}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            #print(f\"File loaded successfully with shape: {df.shape}\")\n",
    "            \n",
    "            velocity = df['velocity(m/s)'].values\n",
    "            time_rel = df['time_rel(sec)'].values\n",
    "            #print(f\"Velocity data shape: {velocity.shape}\")\n",
    "            \n",
    "            features = self.extract_features(velocity, time_rel)\n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"\n",
    "        Prepare the complete dataset by processing all files and matching with catalog.\n",
    "        Returns:\n",
    "            tuple: (X, y, processed_files) - features, labels, and list of processed filenames\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Preparing Dataset ===\")\n",
    "        X = []\n",
    "        y = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Get list of CSV files\n",
    "        csv_files = [f for f in os.listdir(self.data_dir) if f.endswith('.csv')]\n",
    "        #print(f\"Found {len(csv_files)} CSV files in directory\")\n",
    "        \n",
    "        # Debug: Print first few entries of the catalog\n",
    "        #print(\"\\nFirst few catalog entries:\")\n",
    "        #print(self.catalog['filename'].head())\n",
    "        \n",
    "        for filename in csv_files:\n",
    "            # Normalize filename (remove extension)\n",
    "            normalized_filename = filename.replace('.csv', '')\n",
    "            #print(f\"\\nProcessing {filename} (normalized: {normalized_filename})\")\n",
    "            \n",
    "            # Debug: Print exact matching condition\n",
    "            matching_entries = self.catalog[self.catalog['filename'].str.contains(normalized_filename, regex=False)]\n",
    "            #print(f\"Number of matching entries found: {len(matching_entries)}\")\n",
    "            \n",
    "            if len(matching_entries) > 0:\n",
    "                #print(f\"Found catalog entry for {filename}\")\n",
    "                file_path = os.path.join(self.data_dir, filename)\n",
    "                features = self.process_single_file(file_path)\n",
    "                \n",
    "                if features is not None:\n",
    "                    X.append(features)\n",
    "                    y.append(matching_entries['mq_type'].iloc[0])\n",
    "                    processed_files.append(filename)\n",
    "                    #print(f\"Successfully processed {filename}\")\n",
    "            else:\n",
    "                print(f\"No catalog entry found for {filename}\")\n",
    "        \n",
    "        # Check if dataset is empty after processing all files\n",
    "        if not X:\n",
    "            print(\"No matching files found in the catalog. Dataset is empty.\")\n",
    "            return np.array([]), np.array([]), []\n",
    "        \n",
    "        # Convert to numpy arrays after processing all files\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        return X, y, processed_files\n",
    "\n",
    "    \n",
    "    def train_and_evaluate(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"Train and evaluate the classifier.\"\"\"\n",
    "        print(\"\\n=== Training and Evaluation ===\")\n",
    "        try:\n",
    "            # Prepare dataset\n",
    "            X, y, processed_files = self.prepare_dataset()\n",
    "            \n",
    "            # Print dataset statistics\n",
    "            print(\"\\nDataset Statistics:\")\n",
    "            print(f\"Total samples: {len(X)}\")\n",
    "            print(\"\\nClass distribution:\")\n",
    "            unique_classes, counts = np.unique(y, return_counts=True)\n",
    "            for cls, count in zip(unique_classes, counts):\n",
    "                print(f\"{cls}: {count}\")\n",
    "            \n",
    "            # Encode labels\n",
    "            y_encoded = self.label_encoder.fit_transform(y)\n",
    "            print(f\"Encoded {len(unique_classes)} classes\")\n",
    "            \n",
    "            # Split dataset\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded\n",
    "            )\n",
    "            print(f\"\\nTrain set size: {len(X_train)}\")\n",
    "            print(f\"Test set size: {len(X_test)}\")\n",
    "            \n",
    "            # Scale features\n",
    "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "            X_test_scaled = self.scaler.transform(X_test)\n",
    "            \n",
    "            # Train and evaluate models\n",
    "            models = {\n",
    "                'Random Forest': RandomForestClassifier(\n",
    "                    n_estimators=100, \n",
    "                    max_depth=10,\n",
    "                    random_state=random_state\n",
    "                ),\n",
    "                'Neural Network': MLPClassifier(\n",
    "                    hidden_layer_sizes=(100, 50),\n",
    "                    max_iter=1000,\n",
    "                    random_state=random_state\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            results = {}\n",
    "            for name, model in models.items():\n",
    "                print(f\"\\nTraining {name}...\")\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                print(f\"{name} training completed\")\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                \n",
    "                # Print classification report\n",
    "                print(f\"\\nClassification Report for {name}:\")\n",
    "                report = classification_report(\n",
    "                    y_test,\n",
    "                    y_pred,\n",
    "                    target_names=self.label_encoder.classes_\n",
    "                )\n",
    "                print(f\"\\n{report}\")\n",
    "                \n",
    "                results[name] = {\n",
    "                    'model': model,\n",
    "                    'predictions': y_pred,\n",
    "                    'report': report\n",
    "                }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in training and evaluation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the moonquake classification pipeline.\"\"\"\n",
    "    try:\n",
    "        # Define paths\n",
    "        data_dir = 'C:\\\\Users\\\\DELL\\\\Desktop\\\\New folder\\\\space_apps_2024_seismic_detection\\\\data\\\\lunar\\\\training\\\\data\\\\S12_GradeA'\n",
    "        catalog_path = 'C:\\\\Users\\\\DELL\\\\Desktop\\\\New folder\\\\space_apps_2024_seismic_detection\\\\data\\\\lunar\\\\training\\\\catalogs\\\\apollo12_catalog_GradeA_final.csv'\n",
    "\n",
    "\n",
    "        \n",
    "        # Test paths before proceeding\n",
    "        # print(\"\\n=== Testing Data Paths ===\")\n",
    "        test_data_paths(data_dir, catalog_path)\n",
    "        \n",
    "        # Create instance of classifier\n",
    "        # print(\"\\n=== Creating Classifier Instance ===\")\n",
    "        classifier = MoonquakeClassifier(data_dir, catalog_path)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        # print(\"\\n=== Starting Training and Evaluation ===\")\n",
    "        results = classifier.train_and_evaluate()\n",
    "        \n",
    "        # Print final results\n",
    "        print(\"\\n=== Final Results ===\")\n",
    "        for model_name, model_results in results.items():\n",
    "            print(f\"\\nResults for {model_name}:\")\n",
    "            print(f\"Model: {model_results['model']}\")\n",
    "            print(f\"Classification Report:\\n{model_results['report']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred in main: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def process_uploads(catalog_file, data_folder):\n",
    "    \"\"\"\n",
    "    Process uploaded files and return classification results\n",
    "    \n",
    "    Args:\n",
    "        catalog_file: Uploaded catalog CSV file\n",
    "        data_folder: Uploaded folder containing CSV data files\n",
    "    \n",
    "    Returns:\n",
    "        str: Classification results as formatted string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a temporary directory to store uploaded files\n",
    "        with TemporaryDirectory() as temp_dir:\n",
    "            # Save the catalog file\n",
    "            catalog_path = os.path.join(temp_dir, \"catalog.csv\")\n",
    "            shutil.copy2(catalog_file, catalog_path)\n",
    "            \n",
    "            # Create a directory for the data files\n",
    "            data_dir = os.path.join(temp_dir, \"data\")\n",
    "            os.makedirs(data_dir, exist_ok=True)\n",
    "            \n",
    "            # Extract and save all files from the uploaded folder\n",
    "            for file_name in os.listdir(data_folder):\n",
    "                if file_name.endswith('.csv'):\n",
    "                    src_path = os.path.join(data_folder, file_name)\n",
    "                    dst_path = os.path.join(data_dir, file_name)\n",
    "                    shutil.copy2(src_path, dst_path)\n",
    "            \n",
    "            # Test paths\n",
    "            test_data_paths(data_dir, catalog_path)\n",
    "            \n",
    "            # Initialize and run classifier\n",
    "            classifier = MoonquakeClassifier(data_dir, catalog_path)\n",
    "            results = classifier.train_and_evaluate()\n",
    "            \n",
    "            # Format results as string\n",
    "            output = \"=== Classification Results ===\\n\\n\"\n",
    "            for model_name, model_results in results.items():\n",
    "                output += f\"Results for {model_name}:\\n\"\n",
    "                output += f\"Classification Report:\\n{model_results['report']}\\n\"\n",
    "                output += \"-\" * 50 + \"\\n\"\n",
    "            \n",
    "            return output\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\\n{str(traceback.format_exc())}\"\n",
    "\n",
    "# Create Gradio interface\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks(title=\"Moonquake Classification Interface\") as interface:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # Moonquake Classification Interface\n",
    "        \n",
    "        Upload your catalog file and data folder to classify moonquake data.\n",
    "        \n",
    "        **Instructions:**\n",
    "        1. Upload a CSV catalog file\n",
    "        2. Select a folder containing your moonquake data CSV files\n",
    "        3. Click 'Submit' to process the data and view results\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                catalog_input = gr.File(\n",
    "                    label=\"Upload Catalog CSV\",\n",
    "                    file_types=[\".csv\"],\n",
    "                    type=\"filepath\"  # Changed from 'file' to 'filepath'\n",
    "                )\n",
    "                data_folder_input = gr.File(\n",
    "                    label=\"Upload Data Folder\",\n",
    "                    file_count=\"directory\",\n",
    "                    type=\"filepath\"  # Changed from 'file' to 'filepath'\n",
    "                )\n",
    "                submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            \n",
    "            with gr.Column():\n",
    "                output_text = gr.TextArea(\n",
    "                    label=\"Classification Results\",\n",
    "                    placeholder=\"Results will appear here...\",\n",
    "                    lines=20,\n",
    "                    max_lines=30\n",
    "                )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            fn=process_uploads,\n",
    "            inputs=[catalog_input, data_folder_input],\n",
    "            outputs=output_text\n",
    "        )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    interface = create_gradio_interface()\n",
    "    interface.launch(share=True, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
